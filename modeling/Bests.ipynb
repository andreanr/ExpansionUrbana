{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-colorblind')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_cases(metrics):\n",
    "    cases = []\n",
    "    for metric in metrics:\n",
    "        new_name = metric.replace(\" \", \"_\").replace('@', '')\n",
    "        cases.append(\"max(CASE WHEN metric = '{0}' then round(value::numeric,2) else null END) as {1}\".format(metric,new_name))\n",
    "    return \", \".join(cases)\n",
    "\n",
    "\n",
    "def table_winners(metrics, winners):\n",
    "    cases = gen_cases(metrics)\n",
    "    \n",
    "    queries = []\n",
    "    for model, vals in winners.items():\n",
    "        query = (\"\"\"(WITH best{model} AS (\n",
    "                      SELECT model_parameters\n",
    "                      FROM results.models \n",
    "                      WHERE model_id = {id})\n",
    "                        SELECT \n",
    "                           -- model_id,\n",
    "                           model_type,\n",
    "                           model_parameters,\n",
    "                           grid_size,\n",
    "                           year_train,\n",
    "                           year_test,\n",
    "                           cutoff,\n",
    "                           {cases}\n",
    "                    FROM results.models\n",
    "                    JOIN best{model}\n",
    "                        USING (model_parameters)\n",
    "                    JOIN results.evaluations\n",
    "                        USING (model_id)\n",
    "                    WHERE cutoff = '{cutoff}'\n",
    "                    AND ((year_train ='2000' and year_test = 2005) OR\n",
    "                        (year_train = '2005' and year_test = 2010))\n",
    "                    GROUP BY model_id,\n",
    "                            model_type,\n",
    "                            model_parameters,\n",
    "                            grid_size,\n",
    "                            year_train,\n",
    "                            year_test,\n",
    "                            cutoff)\"\"\".format(cases=cases,\n",
    "                                                 model=model.lower(),\n",
    "                                                id = vals['model_id'],\n",
    "                                                 cutoff = vals['cutoff']))\n",
    "        queries.append(query)\n",
    "    query_complete =  \" UNION \".join(queries)\n",
    "    db_engine = utils.get_connection()\n",
    "    data = pd.read_sql(query_complete, db_engine)   \n",
    "    return data\n",
    "\n",
    "def get_metres(grid_size):\n",
    "    if grid_size == 'hex_grid_1000':\n",
    "        return 40594.94\n",
    "    elif grid_size == 'hex_grid_500':\n",
    "        return 187500\n",
    "    elif grid_size == 'hex_grid_250':\n",
    "        return 750000\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def normalize_costs(size, year_test, costs):\n",
    "    if size == 'hex_grid_1000':\n",
    "        if year_test == 2005:\n",
    "            max_costs = 79176\n",
    "        else:\n",
    "            max_costs = 78561\n",
    "            \n",
    "    elif size == 'hex_grid_500':\n",
    "        if year_test == 2005:\n",
    "            max_costs = 239703\n",
    "        else:\n",
    "            max_costs = 241708\n",
    "            \n",
    "    else:\n",
    "        if year_test == 2005:\n",
    "            max_costs = 849968\n",
    "        else:\n",
    "            max_costs = 759175\n",
    "    return costs / max_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "metrics = ['recall ch@', 'precision ch@', 'cost_loss@', 'savings@', 'f1 ch@']\n",
    "\n",
    "winners = {'RandomForest': {\n",
    "                'model_id': 42, \n",
    "                'cutoff': '0.3'\n",
    "              },\n",
    "            'LogisticRegression': {\n",
    "                'model_id': 195,\n",
    "                'cutoff': '0.35'\n",
    "              },\n",
    "            'CostSensitiveRandomForest': {\n",
    "                'model_id': 225,\n",
    "                'cutoff': '0.3'\n",
    "             },\n",
    "           'CostSensitiveLogisticRegression': {\n",
    "               'model_id': 1445,\n",
    "               'cutoff': '0.45'\n",
    "           }\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = table_winners(metrics, winners)\n",
    "res['meters'] = res['grid_size'].map(get_metres)\n",
    "#res['cost_loss_m2'] = res['cost_loss'] / res['meters']\n",
    "res['costs_normalize'] = res.apply(lambda x: normalize_costs(x['grid_size'],\n",
    "                                                             x['year_test'],\n",
    "                                                             x['cost_loss']), axis=1)\n",
    "\n",
    "sortbys = ['model_type', 'meters', 'year_train', 'year_test']\n",
    "res = res.sort_values(by=sortbys)\n",
    "res.to_csv('resultados1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_type</th>\n",
       "      <th>model_parameters</th>\n",
       "      <th>grid_size</th>\n",
       "      <th>year_train</th>\n",
       "      <th>year_test</th>\n",
       "      <th>cutoff</th>\n",
       "      <th>recall_ch</th>\n",
       "      <th>precision_ch</th>\n",
       "      <th>cost_loss</th>\n",
       "      <th>savings</th>\n",
       "      <th>f1_ch</th>\n",
       "      <th>meters</th>\n",
       "      <th>costs_normalize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CostSensitiveLogisticRegression</td>\n",
       "      <td>{'C': 5, 'tol': 0.01, 'solver': 'ga', 'max_ite...</td>\n",
       "      <td>hex_grid_1000</td>\n",
       "      <td>2000</td>\n",
       "      <td>2005</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.36</td>\n",
       "      <td>135728.0</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>0.49</td>\n",
       "      <td>40594.94</td>\n",
       "      <td>1.714257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CostSensitiveLogisticRegression</td>\n",
       "      <td>{'C': 5, 'tol': 0.01, 'solver': 'ga', 'max_ite...</td>\n",
       "      <td>hex_grid_1000</td>\n",
       "      <td>2005</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.56</td>\n",
       "      <td>49865.5</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.64</td>\n",
       "      <td>40594.94</td>\n",
       "      <td>0.634736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CostSensitiveLogisticRegression</td>\n",
       "      <td>{'C': 5, 'tol': 0.01, 'solver': 'ga', 'max_ite...</td>\n",
       "      <td>hex_grid_500</td>\n",
       "      <td>2000</td>\n",
       "      <td>2005</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.66</td>\n",
       "      <td>225907.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.26</td>\n",
       "      <td>187500.00</td>\n",
       "      <td>0.942445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CostSensitiveLogisticRegression</td>\n",
       "      <td>{'C': 5, 'tol': 0.01, 'solver': 'ga', 'max_ite...</td>\n",
       "      <td>hex_grid_500</td>\n",
       "      <td>2005</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.54</td>\n",
       "      <td>171096.0</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.55</td>\n",
       "      <td>187500.00</td>\n",
       "      <td>0.707862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CostSensitiveLogisticRegression</td>\n",
       "      <td>{'C': 5, 'tol': 0.01, 'solver': 'ga', 'max_ite...</td>\n",
       "      <td>hex_grid_250</td>\n",
       "      <td>2000</td>\n",
       "      <td>2005</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.61</td>\n",
       "      <td>738018.0</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.37</td>\n",
       "      <td>750000.00</td>\n",
       "      <td>0.868289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CostSensitiveLogisticRegression</td>\n",
       "      <td>{'C': 5, 'tol': 0.01, 'solver': 'ga', 'max_ite...</td>\n",
       "      <td>hex_grid_250</td>\n",
       "      <td>2005</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>635755.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>750000.00</td>\n",
       "      <td>0.837429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CostSensitiveRandomForest</td>\n",
       "      <td>{'pruned': True, 'combination': 'majority_voti...</td>\n",
       "      <td>hex_grid_1000</td>\n",
       "      <td>2000</td>\n",
       "      <td>2005</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.90</td>\n",
       "      <td>35044.1</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.82</td>\n",
       "      <td>40594.94</td>\n",
       "      <td>0.442610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CostSensitiveRandomForest</td>\n",
       "      <td>{'pruned': True, 'combination': 'majority_voti...</td>\n",
       "      <td>hex_grid_1000</td>\n",
       "      <td>2005</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.79</td>\n",
       "      <td>37832.5</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.72</td>\n",
       "      <td>40594.94</td>\n",
       "      <td>0.481568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CostSensitiveRandomForest</td>\n",
       "      <td>{'pruned': True, 'combination': 'majority_voti...</td>\n",
       "      <td>hex_grid_500</td>\n",
       "      <td>2000</td>\n",
       "      <td>2005</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.83</td>\n",
       "      <td>134567.0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.72</td>\n",
       "      <td>187500.00</td>\n",
       "      <td>0.561391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>CostSensitiveRandomForest</td>\n",
       "      <td>{'pruned': True, 'combination': 'majority_voti...</td>\n",
       "      <td>hex_grid_500</td>\n",
       "      <td>2005</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.83</td>\n",
       "      <td>141362.0</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.63</td>\n",
       "      <td>187500.00</td>\n",
       "      <td>0.584846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CostSensitiveRandomForest</td>\n",
       "      <td>{'pruned': True, 'combination': 'majority_voti...</td>\n",
       "      <td>hex_grid_250</td>\n",
       "      <td>2000</td>\n",
       "      <td>2005</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.78</td>\n",
       "      <td>540860.0</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.63</td>\n",
       "      <td>750000.00</td>\n",
       "      <td>0.636330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CostSensitiveRandomForest</td>\n",
       "      <td>{'pruned': True, 'combination': 'majority_voti...</td>\n",
       "      <td>hex_grid_250</td>\n",
       "      <td>2005</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.56</td>\n",
       "      <td>541954.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.48</td>\n",
       "      <td>750000.00</td>\n",
       "      <td>0.713872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>{'C_reg': 1, 'penalty': 'l1', 'random_state': ...</td>\n",
       "      <td>hex_grid_1000</td>\n",
       "      <td>2000</td>\n",
       "      <td>2005</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.89</td>\n",
       "      <td>39109.1</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.79</td>\n",
       "      <td>40594.94</td>\n",
       "      <td>0.493951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>{'C_reg': 1, 'penalty': 'l1', 'random_state': ...</td>\n",
       "      <td>hex_grid_1000</td>\n",
       "      <td>2005</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.70</td>\n",
       "      <td>42539.4</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.69</td>\n",
       "      <td>40594.94</td>\n",
       "      <td>0.541482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>{'C_reg': 1, 'penalty': 'l1', 'random_state': ...</td>\n",
       "      <td>hex_grid_500</td>\n",
       "      <td>2000</td>\n",
       "      <td>2005</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.79</td>\n",
       "      <td>133350.0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.75</td>\n",
       "      <td>187500.00</td>\n",
       "      <td>0.556313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>{'C_reg': 1, 'penalty': 'l1', 'random_state': ...</td>\n",
       "      <td>hex_grid_500</td>\n",
       "      <td>2005</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.62</td>\n",
       "      <td>160533.0</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.57</td>\n",
       "      <td>187500.00</td>\n",
       "      <td>0.664161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>{'C_reg': 1, 'penalty': 'l1', 'random_state': ...</td>\n",
       "      <td>hex_grid_250</td>\n",
       "      <td>2000</td>\n",
       "      <td>2005</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.76</td>\n",
       "      <td>500284.0</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.71</td>\n",
       "      <td>750000.00</td>\n",
       "      <td>0.588592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>{'C_reg': 1, 'penalty': 'l1', 'random_state': ...</td>\n",
       "      <td>hex_grid_250</td>\n",
       "      <td>2005</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.45</td>\n",
       "      <td>589147.0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.44</td>\n",
       "      <td>750000.00</td>\n",
       "      <td>0.776036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'max_fea...</td>\n",
       "      <td>hex_grid_1000</td>\n",
       "      <td>2000</td>\n",
       "      <td>2005</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.88</td>\n",
       "      <td>38800.9</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.79</td>\n",
       "      <td>40594.94</td>\n",
       "      <td>0.490059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'max_fea...</td>\n",
       "      <td>hex_grid_1000</td>\n",
       "      <td>2005</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.79</td>\n",
       "      <td>37832.5</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.72</td>\n",
       "      <td>40594.94</td>\n",
       "      <td>0.481568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'max_fea...</td>\n",
       "      <td>hex_grid_500</td>\n",
       "      <td>2000</td>\n",
       "      <td>2005</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.81</td>\n",
       "      <td>136601.0</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.71</td>\n",
       "      <td>187500.00</td>\n",
       "      <td>0.569876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'max_fea...</td>\n",
       "      <td>hex_grid_500</td>\n",
       "      <td>2005</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.75</td>\n",
       "      <td>145818.0</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.62</td>\n",
       "      <td>187500.00</td>\n",
       "      <td>0.603282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'max_fea...</td>\n",
       "      <td>hex_grid_250</td>\n",
       "      <td>2000</td>\n",
       "      <td>2005</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.78</td>\n",
       "      <td>507716.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.68</td>\n",
       "      <td>750000.00</td>\n",
       "      <td>0.597335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 5, 'max_fea...</td>\n",
       "      <td>hex_grid_250</td>\n",
       "      <td>2005</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.61</td>\n",
       "      <td>521922.0</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.50</td>\n",
       "      <td>750000.00</td>\n",
       "      <td>0.687486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         model_type  \\\n",
       "13  CostSensitiveLogisticRegression   \n",
       "7   CostSensitiveLogisticRegression   \n",
       "19  CostSensitiveLogisticRegression   \n",
       "11  CostSensitiveLogisticRegression   \n",
       "2   CostSensitiveLogisticRegression   \n",
       "0   CostSensitiveLogisticRegression   \n",
       "5         CostSensitiveRandomForest   \n",
       "1         CostSensitiveRandomForest   \n",
       "15        CostSensitiveRandomForest   \n",
       "23        CostSensitiveRandomForest   \n",
       "4         CostSensitiveRandomForest   \n",
       "8         CostSensitiveRandomForest   \n",
       "12               LogisticRegression   \n",
       "22               LogisticRegression   \n",
       "6                LogisticRegression   \n",
       "20               LogisticRegression   \n",
       "9                LogisticRegression   \n",
       "3                LogisticRegression   \n",
       "16                     RandomForest   \n",
       "21                     RandomForest   \n",
       "17                     RandomForest   \n",
       "14                     RandomForest   \n",
       "18                     RandomForest   \n",
       "10                     RandomForest   \n",
       "\n",
       "                                     model_parameters      grid_size  \\\n",
       "13  {'C': 5, 'tol': 0.01, 'solver': 'ga', 'max_ite...  hex_grid_1000   \n",
       "7   {'C': 5, 'tol': 0.01, 'solver': 'ga', 'max_ite...  hex_grid_1000   \n",
       "19  {'C': 5, 'tol': 0.01, 'solver': 'ga', 'max_ite...   hex_grid_500   \n",
       "11  {'C': 5, 'tol': 0.01, 'solver': 'ga', 'max_ite...   hex_grid_500   \n",
       "2   {'C': 5, 'tol': 0.01, 'solver': 'ga', 'max_ite...   hex_grid_250   \n",
       "0   {'C': 5, 'tol': 0.01, 'solver': 'ga', 'max_ite...   hex_grid_250   \n",
       "5   {'pruned': True, 'combination': 'majority_voti...  hex_grid_1000   \n",
       "1   {'pruned': True, 'combination': 'majority_voti...  hex_grid_1000   \n",
       "15  {'pruned': True, 'combination': 'majority_voti...   hex_grid_500   \n",
       "23  {'pruned': True, 'combination': 'majority_voti...   hex_grid_500   \n",
       "4   {'pruned': True, 'combination': 'majority_voti...   hex_grid_250   \n",
       "8   {'pruned': True, 'combination': 'majority_voti...   hex_grid_250   \n",
       "12  {'C_reg': 1, 'penalty': 'l1', 'random_state': ...  hex_grid_1000   \n",
       "22  {'C_reg': 1, 'penalty': 'l1', 'random_state': ...  hex_grid_1000   \n",
       "6   {'C_reg': 1, 'penalty': 'l1', 'random_state': ...   hex_grid_500   \n",
       "20  {'C_reg': 1, 'penalty': 'l1', 'random_state': ...   hex_grid_500   \n",
       "9   {'C_reg': 1, 'penalty': 'l1', 'random_state': ...   hex_grid_250   \n",
       "3   {'C_reg': 1, 'penalty': 'l1', 'random_state': ...   hex_grid_250   \n",
       "16  {'criterion': 'gini', 'max_depth': 5, 'max_fea...  hex_grid_1000   \n",
       "21  {'criterion': 'gini', 'max_depth': 5, 'max_fea...  hex_grid_1000   \n",
       "17  {'criterion': 'gini', 'max_depth': 5, 'max_fea...   hex_grid_500   \n",
       "14  {'criterion': 'gini', 'max_depth': 5, 'max_fea...   hex_grid_500   \n",
       "18  {'criterion': 'gini', 'max_depth': 5, 'max_fea...   hex_grid_250   \n",
       "10  {'criterion': 'gini', 'max_depth': 5, 'max_fea...   hex_grid_250   \n",
       "\n",
       "   year_train  year_test cutoff  recall_ch  precision_ch  cost_loss  savings  \\\n",
       "13       2000       2005   0.45       0.77          0.36   135728.0    -0.71   \n",
       "7        2005       2010   0.45       0.75          0.56    49865.5     0.23   \n",
       "19       2000       2005   0.45       0.16          0.66   225907.0     0.06   \n",
       "11       2005       2010   0.45       0.56          0.54   171096.0     0.13   \n",
       "2        2000       2005   0.45       0.27          0.61   738018.0     0.13   \n",
       "0        2005       2010   0.45       0.00           NaN   635755.0     0.00   \n",
       "5        2000       2005    0.3       0.76          0.90    35044.1     0.56   \n",
       "1        2005       2010    0.3       0.66          0.79    37832.5     0.42   \n",
       "15       2000       2005    0.3       0.64          0.83   134567.0     0.44   \n",
       "23       2005       2010    0.3       0.51          0.83   141362.0     0.28   \n",
       "4        2000       2005    0.3       0.53          0.78   540860.0     0.36   \n",
       "8        2005       2010    0.3       0.42          0.56   541954.0     0.15   \n",
       "12       2000       2005   0.35       0.71          0.89    39109.1     0.51   \n",
       "22       2005       2010   0.35       0.69          0.70    42539.4     0.35   \n",
       "6        2000       2005   0.35       0.70          0.79   133350.0     0.44   \n",
       "20       2005       2010   0.35       0.53          0.62   160533.0     0.19   \n",
       "9        2000       2005   0.35       0.67          0.76   500284.0     0.41   \n",
       "3        2005       2010   0.35       0.43          0.45   589147.0     0.07   \n",
       "16       2000       2005    0.3       0.72          0.88    38800.9     0.51   \n",
       "21       2005       2010    0.3       0.66          0.79    37832.5     0.42   \n",
       "17       2000       2005    0.3       0.63          0.81   136601.0     0.43   \n",
       "14       2005       2010    0.3       0.53          0.75   145818.0     0.26   \n",
       "18       2000       2005    0.3       0.60          0.78   507716.0     0.40   \n",
       "10       2005       2010    0.3       0.42          0.61   521922.0     0.18   \n",
       "\n",
       "    f1_ch     meters  costs_normalize  \n",
       "13   0.49   40594.94         1.714257  \n",
       "7    0.64   40594.94         0.634736  \n",
       "19   0.26  187500.00         0.942445  \n",
       "11   0.55  187500.00         0.707862  \n",
       "2    0.37  750000.00         0.868289  \n",
       "0    0.00  750000.00         0.837429  \n",
       "5    0.82   40594.94         0.442610  \n",
       "1    0.72   40594.94         0.481568  \n",
       "15   0.72  187500.00         0.561391  \n",
       "23   0.63  187500.00         0.584846  \n",
       "4    0.63  750000.00         0.636330  \n",
       "8    0.48  750000.00         0.713872  \n",
       "12   0.79   40594.94         0.493951  \n",
       "22   0.69   40594.94         0.541482  \n",
       "6    0.75  187500.00         0.556313  \n",
       "20   0.57  187500.00         0.664161  \n",
       "9    0.71  750000.00         0.588592  \n",
       "3    0.44  750000.00         0.776036  \n",
       "16   0.79   40594.94         0.490059  \n",
       "21   0.72   40594.94         0.481568  \n",
       "17   0.71  187500.00         0.569876  \n",
       "14   0.62  187500.00         0.603282  \n",
       "18   0.68  750000.00         0.597335  \n",
       "10   0.50  750000.00         0.687486  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_winners_allcutoffs(winners):\n",
    "    cases = gen_cases(['recall ch@', 'precision ch@'])\n",
    "    \n",
    "    queries = []\n",
    "    for model, vals in winners.items():\n",
    "        query = (\"\"\"best{model} AS (\n",
    "                      SELECT  \n",
    "                              cutoff, \n",
    "                             year_test,\n",
    "                             max(case when metric = 'precision ch@'\n",
    "                                  then value else null end) as {model}_precision,\n",
    "                             max(case when metric = 'recall ch@'\n",
    "                                  then value else null end) as {model}_recall,\n",
    "                             max(case when metric = 'true negatives ch@'\n",
    "                                  then value else null end) as {model}_tn,\n",
    "                             max(case when metric = 'false positives ch@'\n",
    "                                  then value else null end) as {model}_fp\n",
    "                      FROM results.models m1\n",
    "                      join results.models m2\n",
    "                      using (model_parameters)\n",
    "                      join results.evaluations e\n",
    "                       on m2.model_id = e.model_id\n",
    "                      WHERE m1.model_id = {id}\n",
    "                      and m1.model_id != m2.model_id\n",
    "                      and m1.grid_size = 'hex_grid_1000'\n",
    "                      and m2.grid_size = 'hex_grid_1000'\n",
    "                      and m2.year_train = '2005'\n",
    "                      group by cutoff, year_test)\n",
    "                      \"\"\".format(model=model.lower(),\n",
    "                                 id = vals['model_id']))\n",
    "        queries.append(query)\n",
    "    queries_first = \"WITH \" + \", \".join(queries)\n",
    "    \n",
    "    query_second = \"\"\" SELECT * FROM best{model1}\"\"\".format(model1=list(winners.keys())[0])\n",
    "    query_joins = []\n",
    "    for model in list(winners.keys())[1:]:\n",
    "        query= (\"\"\" JOIN best{model2}\n",
    "                           using(cutoff, year_test)\"\"\".format(model2=model))\n",
    "        query_joins.append(query)\n",
    "    \n",
    "    query_final = queries_first + query_second + ' '.join(query_joins)\n",
    "        \n",
    "    db_engine = utils.get_connection()\n",
    "    data = pd.read_sql(query_final, db_engine)   \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cutoff</th>\n",
       "      <th>year_test</th>\n",
       "      <th>randomforest_precision</th>\n",
       "      <th>randomforest_recall</th>\n",
       "      <th>randomforest_tn</th>\n",
       "      <th>randomforest_fp</th>\n",
       "      <th>logisticregression_precision</th>\n",
       "      <th>logisticregression_recall</th>\n",
       "      <th>logisticregression_tn</th>\n",
       "      <th>logisticregression_fp</th>\n",
       "      <th>costsensitiverandomforest_precision</th>\n",
       "      <th>costsensitiverandomforest_recall</th>\n",
       "      <th>costsensitiverandomforest_tn</th>\n",
       "      <th>costsensitiverandomforest_fp</th>\n",
       "      <th>costsensitivelogisticregression_precision</th>\n",
       "      <th>costsensitivelogisticregression_recall</th>\n",
       "      <th>costsensitivelogisticregression_tn</th>\n",
       "      <th>costsensitivelogisticregression_fp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.10</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.670103</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>1638.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.563636</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>1622.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.609091</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>1634.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.15</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.717647</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>1646.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.601942</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>1629.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.685393</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>1642.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>1634.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.20</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>1650.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.626263</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>1633.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.728395</td>\n",
       "      <td>0.710843</td>\n",
       "      <td>1648.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>1634.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.25</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>1651.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.644444</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>1638.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.746667</td>\n",
       "      <td>0.674699</td>\n",
       "      <td>1651.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>1636.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.30</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>1655.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>1642.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>1655.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>1636.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.35</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>1655.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.695122</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>1645.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.808824</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>1657.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>1636.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.40</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.797101</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>1656.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>1646.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.820895</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>1658.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>1636.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.45</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.820895</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>1658.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>1646.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.650602</td>\n",
       "      <td>1658.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>1636.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.50</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.820895</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>1658.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>1648.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>0.650602</td>\n",
       "      <td>1659.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>1636.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.55</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>1660.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>1648.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.841270</td>\n",
       "      <td>0.638554</td>\n",
       "      <td>1660.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>1636.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.60</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.887097</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>1663.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>1650.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.868852</td>\n",
       "      <td>0.638554</td>\n",
       "      <td>1662.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>1636.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.65</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.898305</td>\n",
       "      <td>0.638554</td>\n",
       "      <td>1664.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.753425</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>1652.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.864407</td>\n",
       "      <td>0.614458</td>\n",
       "      <td>1662.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>1636.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.70</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.910714</td>\n",
       "      <td>0.614458</td>\n",
       "      <td>1665.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>1655.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.602410</td>\n",
       "      <td>1664.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.626374</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>1636.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.75</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.602410</td>\n",
       "      <td>1666.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.650602</td>\n",
       "      <td>1658.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>0.590361</td>\n",
       "      <td>1667.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>1637.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.80</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.590361</td>\n",
       "      <td>1668.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>0.650602</td>\n",
       "      <td>1659.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.578313</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>1637.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.85</td>\n",
       "      <td>2010</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.566265</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.841270</td>\n",
       "      <td>0.638554</td>\n",
       "      <td>1660.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.506024</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>1637.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.90</td>\n",
       "      <td>2010</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.530120</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877193</td>\n",
       "      <td>0.602410</td>\n",
       "      <td>1663.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.445783</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>1637.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cutoff  year_test  randomforest_precision  randomforest_recall  \\\n",
       "0     0.10       2010                0.670103             0.783133   \n",
       "1     0.15       2010                0.717647             0.734940   \n",
       "2     0.20       2010                0.743590             0.698795   \n",
       "3     0.25       2010                0.750000             0.686747   \n",
       "4     0.30       2010                0.785714             0.662651   \n",
       "5     0.35       2010                0.785714             0.662651   \n",
       "6     0.40       2010                0.797101             0.662651   \n",
       "7     0.45       2010                0.820895             0.662651   \n",
       "8     0.50       2010                0.820895             0.662651   \n",
       "9     0.55       2010                0.846154             0.662651   \n",
       "10    0.60       2010                0.887097             0.662651   \n",
       "11    0.65       2010                0.898305             0.638554   \n",
       "12    0.70       2010                0.910714             0.614458   \n",
       "13    0.75       2010                0.925926             0.602410   \n",
       "14    0.80       2010                0.960784             0.590361   \n",
       "15    0.85       2010                1.000000             0.566265   \n",
       "16    0.90       2010                1.000000             0.530120   \n",
       "\n",
       "    randomforest_tn  randomforest_fp  logisticregression_precision  \\\n",
       "0            1638.0             32.0                      0.563636   \n",
       "1            1646.0             24.0                      0.601942   \n",
       "2            1650.0             20.0                      0.626263   \n",
       "3            1651.0             19.0                      0.644444   \n",
       "4            1655.0             15.0                      0.674419   \n",
       "5            1655.0             15.0                      0.695122   \n",
       "6            1656.0             14.0                      0.703704   \n",
       "7            1658.0             12.0                      0.703704   \n",
       "8            1658.0             12.0                      0.714286   \n",
       "9            1660.0             10.0                      0.714286   \n",
       "10           1663.0              7.0                      0.733333   \n",
       "11           1664.0              6.0                      0.753425   \n",
       "12           1665.0              5.0                      0.785714   \n",
       "13           1666.0              4.0                      0.818182   \n",
       "14           1668.0              2.0                      0.830769   \n",
       "15           1670.0              0.0                      0.841270   \n",
       "16           1670.0              0.0                      0.877193   \n",
       "\n",
       "    logisticregression_recall  logisticregression_tn  logisticregression_fp  \\\n",
       "0                    0.746988                 1622.0                   48.0   \n",
       "1                    0.746988                 1629.0                   41.0   \n",
       "2                    0.746988                 1633.0                   37.0   \n",
       "3                    0.698795                 1638.0                   32.0   \n",
       "4                    0.698795                 1642.0                   28.0   \n",
       "5                    0.686747                 1645.0                   25.0   \n",
       "6                    0.686747                 1646.0                   24.0   \n",
       "7                    0.686747                 1646.0                   24.0   \n",
       "8                    0.662651                 1648.0                   22.0   \n",
       "9                    0.662651                 1648.0                   22.0   \n",
       "10                   0.662651                 1650.0                   20.0   \n",
       "11                   0.662651                 1652.0                   18.0   \n",
       "12                   0.662651                 1655.0                   15.0   \n",
       "13                   0.650602                 1658.0                   12.0   \n",
       "14                   0.650602                 1659.0                   11.0   \n",
       "15                   0.638554                 1660.0                   10.0   \n",
       "16                   0.602410                 1663.0                    7.0   \n",
       "\n",
       "    costsensitiverandomforest_precision  costsensitiverandomforest_recall  \\\n",
       "0                              0.609091                          0.807229   \n",
       "1                              0.685393                          0.734940   \n",
       "2                              0.728395                          0.710843   \n",
       "3                              0.746667                          0.674699   \n",
       "4                              0.785714                          0.662651   \n",
       "5                              0.808824                          0.662651   \n",
       "6                              0.820895                          0.662651   \n",
       "7                              0.818182                          0.650602   \n",
       "8                              0.830769                          0.650602   \n",
       "9                              0.841270                          0.638554   \n",
       "10                             0.868852                          0.638554   \n",
       "11                             0.864407                          0.614458   \n",
       "12                             0.892857                          0.602410   \n",
       "13                             0.942308                          0.590361   \n",
       "14                             1.000000                          0.578313   \n",
       "15                             1.000000                          0.506024   \n",
       "16                             1.000000                          0.445783   \n",
       "\n",
       "    costsensitiverandomforest_tn  costsensitiverandomforest_fp  \\\n",
       "0                         1627.0                          43.0   \n",
       "1                         1642.0                          28.0   \n",
       "2                         1648.0                          22.0   \n",
       "3                         1651.0                          19.0   \n",
       "4                         1655.0                          15.0   \n",
       "5                         1657.0                          13.0   \n",
       "6                         1658.0                          12.0   \n",
       "7                         1658.0                          12.0   \n",
       "8                         1659.0                          11.0   \n",
       "9                         1660.0                          10.0   \n",
       "10                        1662.0                           8.0   \n",
       "11                        1662.0                           8.0   \n",
       "12                        1664.0                           6.0   \n",
       "13                        1667.0                           3.0   \n",
       "14                        1670.0                           0.0   \n",
       "15                        1670.0                           0.0   \n",
       "16                        1670.0                           0.0   \n",
       "\n",
       "    costsensitivelogisticregression_precision  \\\n",
       "0                                    0.617021   \n",
       "1                                    0.617021   \n",
       "2                                    0.617021   \n",
       "3                                    0.630435   \n",
       "4                                    0.630435   \n",
       "5                                    0.630435   \n",
       "6                                    0.630435   \n",
       "7                                    0.630435   \n",
       "8                                    0.630435   \n",
       "9                                    0.630435   \n",
       "10                                   0.630435   \n",
       "11                                   0.630435   \n",
       "12                                   0.626374   \n",
       "13                                   0.633333   \n",
       "14                                   0.633333   \n",
       "15                                   0.633333   \n",
       "16                                   0.633333   \n",
       "\n",
       "    costsensitivelogisticregression_recall  \\\n",
       "0                                 0.698795   \n",
       "1                                 0.698795   \n",
       "2                                 0.698795   \n",
       "3                                 0.698795   \n",
       "4                                 0.698795   \n",
       "5                                 0.698795   \n",
       "6                                 0.698795   \n",
       "7                                 0.698795   \n",
       "8                                 0.698795   \n",
       "9                                 0.698795   \n",
       "10                                0.698795   \n",
       "11                                0.698795   \n",
       "12                                0.686747   \n",
       "13                                0.686747   \n",
       "14                                0.686747   \n",
       "15                                0.686747   \n",
       "16                                0.686747   \n",
       "\n",
       "    costsensitivelogisticregression_tn  costsensitivelogisticregression_fp  \n",
       "0                               1634.0                                36.0  \n",
       "1                               1634.0                                36.0  \n",
       "2                               1634.0                                36.0  \n",
       "3                               1636.0                                34.0  \n",
       "4                               1636.0                                34.0  \n",
       "5                               1636.0                                34.0  \n",
       "6                               1636.0                                34.0  \n",
       "7                               1636.0                                34.0  \n",
       "8                               1636.0                                34.0  \n",
       "9                               1636.0                                34.0  \n",
       "10                              1636.0                                34.0  \n",
       "11                              1636.0                                34.0  \n",
       "12                              1636.0                                34.0  \n",
       "13                              1637.0                                33.0  \n",
       "14                              1637.0                                33.0  \n",
       "15                              1637.0                                33.0  \n",
       "16                              1637.0                                33.0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_preds = table_winners_allcutoffs(winners)\n",
    "res_preds['cutoff'] = res_preds['cutoff'].map(float)\n",
    "res_preds['randomforest_recall'] = res_preds['randomforest_recall'].map(float)\n",
    "res_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cutoff</th>\n",
       "      <th>year_test</th>\n",
       "      <th>randomforest_precision</th>\n",
       "      <th>randomforest_recall</th>\n",
       "      <th>randomforest_tn</th>\n",
       "      <th>randomforest_fp</th>\n",
       "      <th>logisticregression_precision</th>\n",
       "      <th>logisticregression_recall</th>\n",
       "      <th>logisticregression_tn</th>\n",
       "      <th>logisticregression_fp</th>\n",
       "      <th>...</th>\n",
       "      <th>costsensitiverandomforest_tn</th>\n",
       "      <th>costsensitiverandomforest_fp</th>\n",
       "      <th>costsensitivelogisticregression_precision</th>\n",
       "      <th>costsensitivelogisticregression_recall</th>\n",
       "      <th>costsensitivelogisticregression_tn</th>\n",
       "      <th>costsensitivelogisticregression_fp</th>\n",
       "      <th>randomforest_specificity</th>\n",
       "      <th>logisticregression_specificity</th>\n",
       "      <th>costsensitiverandomforest_specificity</th>\n",
       "      <th>costsensitivelogisticregression_specificity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.10</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.670103</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>1638.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.563636</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>1622.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>1634.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.019162</td>\n",
       "      <td>0.028743</td>\n",
       "      <td>0.025749</td>\n",
       "      <td>0.021557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.15</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.717647</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>1646.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.601942</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>1629.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1642.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>1634.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.014371</td>\n",
       "      <td>0.024551</td>\n",
       "      <td>0.016766</td>\n",
       "      <td>0.021557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.20</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>1650.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.626263</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>1633.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1648.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>1634.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.011976</td>\n",
       "      <td>0.022156</td>\n",
       "      <td>0.013174</td>\n",
       "      <td>0.021557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.25</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>1651.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.644444</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>1638.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1651.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>1636.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.011377</td>\n",
       "      <td>0.019162</td>\n",
       "      <td>0.011377</td>\n",
       "      <td>0.020359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.30</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>1655.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>1642.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1655.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>1636.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.008982</td>\n",
       "      <td>0.016766</td>\n",
       "      <td>0.008982</td>\n",
       "      <td>0.020359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.35</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>1655.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.695122</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>1645.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1657.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>1636.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.008982</td>\n",
       "      <td>0.014970</td>\n",
       "      <td>0.007784</td>\n",
       "      <td>0.020359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.40</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.797101</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>1656.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>1646.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1658.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>1636.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.008383</td>\n",
       "      <td>0.014371</td>\n",
       "      <td>0.007186</td>\n",
       "      <td>0.020359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.45</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.820895</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>1658.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>1646.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1658.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>1636.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.007186</td>\n",
       "      <td>0.014371</td>\n",
       "      <td>0.007186</td>\n",
       "      <td>0.020359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.50</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.820895</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>1658.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>1648.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1659.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>1636.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.007186</td>\n",
       "      <td>0.013174</td>\n",
       "      <td>0.006587</td>\n",
       "      <td>0.020359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.55</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>1660.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>1648.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1660.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>1636.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.005988</td>\n",
       "      <td>0.013174</td>\n",
       "      <td>0.005988</td>\n",
       "      <td>0.020359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.60</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.887097</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>1663.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>1650.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1662.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>1636.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.004192</td>\n",
       "      <td>0.011976</td>\n",
       "      <td>0.004790</td>\n",
       "      <td>0.020359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.65</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.898305</td>\n",
       "      <td>0.638554</td>\n",
       "      <td>1664.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.753425</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>1652.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1662.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>1636.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.003593</td>\n",
       "      <td>0.010778</td>\n",
       "      <td>0.004790</td>\n",
       "      <td>0.020359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.70</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.910714</td>\n",
       "      <td>0.614458</td>\n",
       "      <td>1665.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>1655.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1664.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.626374</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>1636.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.002994</td>\n",
       "      <td>0.008982</td>\n",
       "      <td>0.003593</td>\n",
       "      <td>0.020359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.75</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.602410</td>\n",
       "      <td>1666.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.650602</td>\n",
       "      <td>1658.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1667.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>1637.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>0.007186</td>\n",
       "      <td>0.001796</td>\n",
       "      <td>0.019760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.80</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.590361</td>\n",
       "      <td>1668.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>0.650602</td>\n",
       "      <td>1659.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>1637.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.001198</td>\n",
       "      <td>0.006587</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.85</td>\n",
       "      <td>2010</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.566265</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.841270</td>\n",
       "      <td>0.638554</td>\n",
       "      <td>1660.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>1637.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.90</td>\n",
       "      <td>2010</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.530120</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877193</td>\n",
       "      <td>0.602410</td>\n",
       "      <td>1663.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>1637.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17 rows  22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    cutoff  year_test  randomforest_precision  randomforest_recall  \\\n",
       "0     0.10       2010                0.670103             0.783133   \n",
       "1     0.15       2010                0.717647             0.734940   \n",
       "2     0.20       2010                0.743590             0.698795   \n",
       "3     0.25       2010                0.750000             0.686747   \n",
       "4     0.30       2010                0.785714             0.662651   \n",
       "5     0.35       2010                0.785714             0.662651   \n",
       "6     0.40       2010                0.797101             0.662651   \n",
       "7     0.45       2010                0.820895             0.662651   \n",
       "8     0.50       2010                0.820895             0.662651   \n",
       "9     0.55       2010                0.846154             0.662651   \n",
       "10    0.60       2010                0.887097             0.662651   \n",
       "11    0.65       2010                0.898305             0.638554   \n",
       "12    0.70       2010                0.910714             0.614458   \n",
       "13    0.75       2010                0.925926             0.602410   \n",
       "14    0.80       2010                0.960784             0.590361   \n",
       "15    0.85       2010                1.000000             0.566265   \n",
       "16    0.90       2010                1.000000             0.530120   \n",
       "\n",
       "    randomforest_tn  randomforest_fp  logisticregression_precision  \\\n",
       "0            1638.0             32.0                      0.563636   \n",
       "1            1646.0             24.0                      0.601942   \n",
       "2            1650.0             20.0                      0.626263   \n",
       "3            1651.0             19.0                      0.644444   \n",
       "4            1655.0             15.0                      0.674419   \n",
       "5            1655.0             15.0                      0.695122   \n",
       "6            1656.0             14.0                      0.703704   \n",
       "7            1658.0             12.0                      0.703704   \n",
       "8            1658.0             12.0                      0.714286   \n",
       "9            1660.0             10.0                      0.714286   \n",
       "10           1663.0              7.0                      0.733333   \n",
       "11           1664.0              6.0                      0.753425   \n",
       "12           1665.0              5.0                      0.785714   \n",
       "13           1666.0              4.0                      0.818182   \n",
       "14           1668.0              2.0                      0.830769   \n",
       "15           1670.0              0.0                      0.841270   \n",
       "16           1670.0              0.0                      0.877193   \n",
       "\n",
       "    logisticregression_recall  logisticregression_tn  logisticregression_fp  \\\n",
       "0                    0.746988                 1622.0                   48.0   \n",
       "1                    0.746988                 1629.0                   41.0   \n",
       "2                    0.746988                 1633.0                   37.0   \n",
       "3                    0.698795                 1638.0                   32.0   \n",
       "4                    0.698795                 1642.0                   28.0   \n",
       "5                    0.686747                 1645.0                   25.0   \n",
       "6                    0.686747                 1646.0                   24.0   \n",
       "7                    0.686747                 1646.0                   24.0   \n",
       "8                    0.662651                 1648.0                   22.0   \n",
       "9                    0.662651                 1648.0                   22.0   \n",
       "10                   0.662651                 1650.0                   20.0   \n",
       "11                   0.662651                 1652.0                   18.0   \n",
       "12                   0.662651                 1655.0                   15.0   \n",
       "13                   0.650602                 1658.0                   12.0   \n",
       "14                   0.650602                 1659.0                   11.0   \n",
       "15                   0.638554                 1660.0                   10.0   \n",
       "16                   0.602410                 1663.0                    7.0   \n",
       "\n",
       "                       ...                       costsensitiverandomforest_tn  \\\n",
       "0                      ...                                             1627.0   \n",
       "1                      ...                                             1642.0   \n",
       "2                      ...                                             1648.0   \n",
       "3                      ...                                             1651.0   \n",
       "4                      ...                                             1655.0   \n",
       "5                      ...                                             1657.0   \n",
       "6                      ...                                             1658.0   \n",
       "7                      ...                                             1658.0   \n",
       "8                      ...                                             1659.0   \n",
       "9                      ...                                             1660.0   \n",
       "10                     ...                                             1662.0   \n",
       "11                     ...                                             1662.0   \n",
       "12                     ...                                             1664.0   \n",
       "13                     ...                                             1667.0   \n",
       "14                     ...                                             1670.0   \n",
       "15                     ...                                             1670.0   \n",
       "16                     ...                                             1670.0   \n",
       "\n",
       "    costsensitiverandomforest_fp  costsensitivelogisticregression_precision  \\\n",
       "0                           43.0                                   0.617021   \n",
       "1                           28.0                                   0.617021   \n",
       "2                           22.0                                   0.617021   \n",
       "3                           19.0                                   0.630435   \n",
       "4                           15.0                                   0.630435   \n",
       "5                           13.0                                   0.630435   \n",
       "6                           12.0                                   0.630435   \n",
       "7                           12.0                                   0.630435   \n",
       "8                           11.0                                   0.630435   \n",
       "9                           10.0                                   0.630435   \n",
       "10                           8.0                                   0.630435   \n",
       "11                           8.0                                   0.630435   \n",
       "12                           6.0                                   0.626374   \n",
       "13                           3.0                                   0.633333   \n",
       "14                           0.0                                   0.633333   \n",
       "15                           0.0                                   0.633333   \n",
       "16                           0.0                                   0.633333   \n",
       "\n",
       "    costsensitivelogisticregression_recall  \\\n",
       "0                                 0.698795   \n",
       "1                                 0.698795   \n",
       "2                                 0.698795   \n",
       "3                                 0.698795   \n",
       "4                                 0.698795   \n",
       "5                                 0.698795   \n",
       "6                                 0.698795   \n",
       "7                                 0.698795   \n",
       "8                                 0.698795   \n",
       "9                                 0.698795   \n",
       "10                                0.698795   \n",
       "11                                0.698795   \n",
       "12                                0.686747   \n",
       "13                                0.686747   \n",
       "14                                0.686747   \n",
       "15                                0.686747   \n",
       "16                                0.686747   \n",
       "\n",
       "    costsensitivelogisticregression_tn  costsensitivelogisticregression_fp  \\\n",
       "0                               1634.0                                36.0   \n",
       "1                               1634.0                                36.0   \n",
       "2                               1634.0                                36.0   \n",
       "3                               1636.0                                34.0   \n",
       "4                               1636.0                                34.0   \n",
       "5                               1636.0                                34.0   \n",
       "6                               1636.0                                34.0   \n",
       "7                               1636.0                                34.0   \n",
       "8                               1636.0                                34.0   \n",
       "9                               1636.0                                34.0   \n",
       "10                              1636.0                                34.0   \n",
       "11                              1636.0                                34.0   \n",
       "12                              1636.0                                34.0   \n",
       "13                              1637.0                                33.0   \n",
       "14                              1637.0                                33.0   \n",
       "15                              1637.0                                33.0   \n",
       "16                              1637.0                                33.0   \n",
       "\n",
       "    randomforest_specificity  logisticregression_specificity  \\\n",
       "0                   0.019162                        0.028743   \n",
       "1                   0.014371                        0.024551   \n",
       "2                   0.011976                        0.022156   \n",
       "3                   0.011377                        0.019162   \n",
       "4                   0.008982                        0.016766   \n",
       "5                   0.008982                        0.014970   \n",
       "6                   0.008383                        0.014371   \n",
       "7                   0.007186                        0.014371   \n",
       "8                   0.007186                        0.013174   \n",
       "9                   0.005988                        0.013174   \n",
       "10                  0.004192                        0.011976   \n",
       "11                  0.003593                        0.010778   \n",
       "12                  0.002994                        0.008982   \n",
       "13                  0.002395                        0.007186   \n",
       "14                  0.001198                        0.006587   \n",
       "15                  0.000000                        0.005988   \n",
       "16                  0.000000                        0.004192   \n",
       "\n",
       "    costsensitiverandomforest_specificity  \\\n",
       "0                                0.025749   \n",
       "1                                0.016766   \n",
       "2                                0.013174   \n",
       "3                                0.011377   \n",
       "4                                0.008982   \n",
       "5                                0.007784   \n",
       "6                                0.007186   \n",
       "7                                0.007186   \n",
       "8                                0.006587   \n",
       "9                                0.005988   \n",
       "10                               0.004790   \n",
       "11                               0.004790   \n",
       "12                               0.003593   \n",
       "13                               0.001796   \n",
       "14                               0.000000   \n",
       "15                               0.000000   \n",
       "16                               0.000000   \n",
       "\n",
       "    costsensitivelogisticregression_specificity  \n",
       "0                                      0.021557  \n",
       "1                                      0.021557  \n",
       "2                                      0.021557  \n",
       "3                                      0.020359  \n",
       "4                                      0.020359  \n",
       "5                                      0.020359  \n",
       "6                                      0.020359  \n",
       "7                                      0.020359  \n",
       "8                                      0.020359  \n",
       "9                                      0.020359  \n",
       "10                                     0.020359  \n",
       "11                                     0.020359  \n",
       "12                                     0.020359  \n",
       "13                                     0.019760  \n",
       "14                                     0.019760  \n",
       "15                                     0.019760  \n",
       "16                                     0.019760  \n",
       "\n",
       "[17 rows x 22 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def specificity(row, n):\n",
    "    #print(row)\n",
    "    sp = row.loc['{}_tn'.format(n)] / (row.loc['{}_tn'.format(n)]+ row.loc['{}_fp'.format(n)] )\n",
    "    return  1-sp\n",
    "\n",
    "res_preds['randomforest_specificity'] = res_preds.apply(lambda row: specificity(row, 'randomforest'), axis=1)\n",
    "res_preds['logisticregression_specificity'] = res_preds.apply(lambda x: specificity(x, 'logisticregression'), axis=1)\n",
    "res_preds['costsensitiverandomforest_specificity'] = res_preds.apply(lambda x: specificity(x, 'costsensitiverandomforest'), axis=1)\n",
    "res_preds['costsensitivelogisticregression_specificity'] = res_preds.apply(lambda x: specificity(x, 'costsensitivelogisticregression'), axis=1)\n",
    "res_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cutoff</th>\n",
       "      <th>randomforest_precision</th>\n",
       "      <th>randomforest_recall</th>\n",
       "      <th>logisticregression_precision</th>\n",
       "      <th>logisticregression_recall</th>\n",
       "      <th>costsensitiverandomforest_precision</th>\n",
       "      <th>costsensitiverandomforest_recall</th>\n",
       "      <th>randomforest_specificity</th>\n",
       "      <th>logisticregression_specificity</th>\n",
       "      <th>costsensitiverandomforest_specificity</th>\n",
       "      <th>costsensitivelogisticregression_precision</th>\n",
       "      <th>costsensitivelogisticregression_recall</th>\n",
       "      <th>costsensitivelogisticregression_specificity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.670103</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.563636</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.609091</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.019162</td>\n",
       "      <td>0.028743</td>\n",
       "      <td>0.025749</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.021557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.717647</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.601942</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.685393</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.014371</td>\n",
       "      <td>0.024551</td>\n",
       "      <td>0.016766</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.021557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.626263</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.728395</td>\n",
       "      <td>0.710843</td>\n",
       "      <td>0.011976</td>\n",
       "      <td>0.022156</td>\n",
       "      <td>0.013174</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.021557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.644444</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.746667</td>\n",
       "      <td>0.674699</td>\n",
       "      <td>0.011377</td>\n",
       "      <td>0.019162</td>\n",
       "      <td>0.011377</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.020359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.008982</td>\n",
       "      <td>0.016766</td>\n",
       "      <td>0.008982</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.020359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cutoff  randomforest_precision  randomforest_recall  \\\n",
       "0    0.10                0.670103             0.783133   \n",
       "1    0.15                0.717647             0.734940   \n",
       "2    0.20                0.743590             0.698795   \n",
       "3    0.25                0.750000             0.686747   \n",
       "4    0.30                0.785714             0.662651   \n",
       "\n",
       "   logisticregression_precision  logisticregression_recall  \\\n",
       "0                      0.563636                   0.746988   \n",
       "1                      0.601942                   0.746988   \n",
       "2                      0.626263                   0.746988   \n",
       "3                      0.644444                   0.698795   \n",
       "4                      0.674419                   0.698795   \n",
       "\n",
       "   costsensitiverandomforest_precision  costsensitiverandomforest_recall  \\\n",
       "0                             0.609091                          0.807229   \n",
       "1                             0.685393                          0.734940   \n",
       "2                             0.728395                          0.710843   \n",
       "3                             0.746667                          0.674699   \n",
       "4                             0.785714                          0.662651   \n",
       "\n",
       "   randomforest_specificity  logisticregression_specificity  \\\n",
       "0                  0.019162                        0.028743   \n",
       "1                  0.014371                        0.024551   \n",
       "2                  0.011976                        0.022156   \n",
       "3                  0.011377                        0.019162   \n",
       "4                  0.008982                        0.016766   \n",
       "\n",
       "   costsensitiverandomforest_specificity  \\\n",
       "0                               0.025749   \n",
       "1                               0.016766   \n",
       "2                               0.013174   \n",
       "3                               0.011377   \n",
       "4                               0.008982   \n",
       "\n",
       "   costsensitivelogisticregression_precision  \\\n",
       "0                                   0.617021   \n",
       "1                                   0.617021   \n",
       "2                                   0.617021   \n",
       "3                                   0.630435   \n",
       "4                                   0.630435   \n",
       "\n",
       "   costsensitivelogisticregression_recall  \\\n",
       "0                                0.698795   \n",
       "1                                0.698795   \n",
       "2                                0.698795   \n",
       "3                                0.698795   \n",
       "4                                0.698795   \n",
       "\n",
       "   costsensitivelogisticregression_specificity  \n",
       "0                                     0.021557  \n",
       "1                                     0.021557  \n",
       "2                                     0.021557  \n",
       "3                                     0.020359  \n",
       "4                                     0.020359  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars_utiles = ['cutoff', 'randomforest_precision', 'randomforest_recall', 'logisticregression_precision',\n",
    "              'logisticregression_recall', 'costsensitiverandomforest_precision', 'costsensitiverandomforest_recall',\n",
    "              'randomforest_specificity', 'logisticregression_specificity', 'costsensitiverandomforest_specificity',\n",
    "              'costsensitivelogisticregression_precision', 'costsensitivelogisticregression_recall',\n",
    "              'costsensitivelogisticregression_specificity']\n",
    "res_preds = res_preds[vars_utiles]\n",
    "res_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cutoff</th>\n",
       "      <th>randomforest_precision</th>\n",
       "      <th>randomforest_recall</th>\n",
       "      <th>logisticregression_precision</th>\n",
       "      <th>logisticregression_recall</th>\n",
       "      <th>costsensitiverandomforest_precision</th>\n",
       "      <th>costsensitiverandomforest_recall</th>\n",
       "      <th>randomforest_specificity</th>\n",
       "      <th>logisticregression_specificity</th>\n",
       "      <th>costsensitiverandomforest_specificity</th>\n",
       "      <th>costsensitivelogisticregression_precision</th>\n",
       "      <th>costsensitivelogisticregression_recall</th>\n",
       "      <th>costsensitivelogisticregression_specificity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.670103</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.563636</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.609091</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.019162</td>\n",
       "      <td>0.028743</td>\n",
       "      <td>0.025749</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.021557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.717647</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.601942</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.685393</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.014371</td>\n",
       "      <td>0.024551</td>\n",
       "      <td>0.016766</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.021557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.626263</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.728395</td>\n",
       "      <td>0.710843</td>\n",
       "      <td>0.011976</td>\n",
       "      <td>0.022156</td>\n",
       "      <td>0.013174</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.021557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.644444</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.746667</td>\n",
       "      <td>0.674699</td>\n",
       "      <td>0.011377</td>\n",
       "      <td>0.019162</td>\n",
       "      <td>0.011377</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.020359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.008982</td>\n",
       "      <td>0.016766</td>\n",
       "      <td>0.008982</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.020359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.695122</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.808824</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.008982</td>\n",
       "      <td>0.014970</td>\n",
       "      <td>0.007784</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.020359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.797101</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.820895</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.008383</td>\n",
       "      <td>0.014371</td>\n",
       "      <td>0.007186</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.020359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.820895</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.650602</td>\n",
       "      <td>0.007186</td>\n",
       "      <td>0.014371</td>\n",
       "      <td>0.007186</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.020359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.820895</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>0.650602</td>\n",
       "      <td>0.007186</td>\n",
       "      <td>0.013174</td>\n",
       "      <td>0.006587</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.020359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.841270</td>\n",
       "      <td>0.638554</td>\n",
       "      <td>0.005988</td>\n",
       "      <td>0.013174</td>\n",
       "      <td>0.005988</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.020359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.887097</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.868852</td>\n",
       "      <td>0.638554</td>\n",
       "      <td>0.004192</td>\n",
       "      <td>0.011976</td>\n",
       "      <td>0.004790</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.020359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.898305</td>\n",
       "      <td>0.638554</td>\n",
       "      <td>0.753425</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.864407</td>\n",
       "      <td>0.614458</td>\n",
       "      <td>0.003593</td>\n",
       "      <td>0.010778</td>\n",
       "      <td>0.004790</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.020359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.910714</td>\n",
       "      <td>0.614458</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.602410</td>\n",
       "      <td>0.002994</td>\n",
       "      <td>0.008982</td>\n",
       "      <td>0.003593</td>\n",
       "      <td>0.626374</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.020359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.602410</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.650602</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>0.590361</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>0.007186</td>\n",
       "      <td>0.001796</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.019760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.590361</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>0.650602</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.578313</td>\n",
       "      <td>0.001198</td>\n",
       "      <td>0.006587</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.019760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.85</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.566265</td>\n",
       "      <td>0.841270</td>\n",
       "      <td>0.638554</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.506024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.019760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.90</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.530120</td>\n",
       "      <td>0.877193</td>\n",
       "      <td>0.602410</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.445783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.019760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cutoff  randomforest_precision  randomforest_recall  \\\n",
       "19    0.00                0.600000             1.000000   \n",
       "18    0.05                0.600000             0.880000   \n",
       "0     0.10                0.670103             0.783133   \n",
       "1     0.15                0.717647             0.734940   \n",
       "2     0.20                0.743590             0.698795   \n",
       "3     0.25                0.750000             0.686747   \n",
       "4     0.30                0.785714             0.662651   \n",
       "5     0.35                0.785714             0.662651   \n",
       "6     0.40                0.797101             0.662651   \n",
       "7     0.45                0.820895             0.662651   \n",
       "8     0.50                0.820895             0.662651   \n",
       "9     0.55                0.846154             0.662651   \n",
       "10    0.60                0.887097             0.662651   \n",
       "11    0.65                0.898305             0.638554   \n",
       "12    0.70                0.910714             0.614458   \n",
       "13    0.75                0.925926             0.602410   \n",
       "14    0.80                0.960784             0.590361   \n",
       "15    0.85                1.000000             0.566265   \n",
       "16    0.90                1.000000             0.530120   \n",
       "17    1.00                1.000000             0.000000   \n",
       "\n",
       "    logisticregression_precision  logisticregression_recall  \\\n",
       "19                      0.500000                   1.000000   \n",
       "18                      0.530000                   0.850000   \n",
       "0                       0.563636                   0.746988   \n",
       "1                       0.601942                   0.746988   \n",
       "2                       0.626263                   0.746988   \n",
       "3                       0.644444                   0.698795   \n",
       "4                       0.674419                   0.698795   \n",
       "5                       0.695122                   0.686747   \n",
       "6                       0.703704                   0.686747   \n",
       "7                       0.703704                   0.686747   \n",
       "8                       0.714286                   0.662651   \n",
       "9                       0.714286                   0.662651   \n",
       "10                      0.733333                   0.662651   \n",
       "11                      0.753425                   0.662651   \n",
       "12                      0.785714                   0.662651   \n",
       "13                      0.818182                   0.650602   \n",
       "14                      0.830769                   0.650602   \n",
       "15                      0.841270                   0.638554   \n",
       "16                      0.877193                   0.602410   \n",
       "17                      1.000000                   0.000000   \n",
       "\n",
       "    costsensitiverandomforest_precision  costsensitiverandomforest_recall  \\\n",
       "19                             0.550000                          1.000000   \n",
       "18                             0.570000                          0.900000   \n",
       "0                              0.609091                          0.807229   \n",
       "1                              0.685393                          0.734940   \n",
       "2                              0.728395                          0.710843   \n",
       "3                              0.746667                          0.674699   \n",
       "4                              0.785714                          0.662651   \n",
       "5                              0.808824                          0.662651   \n",
       "6                              0.820895                          0.662651   \n",
       "7                              0.818182                          0.650602   \n",
       "8                              0.830769                          0.650602   \n",
       "9                              0.841270                          0.638554   \n",
       "10                             0.868852                          0.638554   \n",
       "11                             0.864407                          0.614458   \n",
       "12                             0.892857                          0.602410   \n",
       "13                             0.942308                          0.590361   \n",
       "14                             1.000000                          0.578313   \n",
       "15                             1.000000                          0.506024   \n",
       "16                             1.000000                          0.445783   \n",
       "17                             1.000000                          0.000000   \n",
       "\n",
       "    randomforest_specificity  logisticregression_specificity  \\\n",
       "19                  1.000000                        1.000000   \n",
       "18                  1.000000                        1.000000   \n",
       "0                   0.019162                        0.028743   \n",
       "1                   0.014371                        0.024551   \n",
       "2                   0.011976                        0.022156   \n",
       "3                   0.011377                        0.019162   \n",
       "4                   0.008982                        0.016766   \n",
       "5                   0.008982                        0.014970   \n",
       "6                   0.008383                        0.014371   \n",
       "7                   0.007186                        0.014371   \n",
       "8                   0.007186                        0.013174   \n",
       "9                   0.005988                        0.013174   \n",
       "10                  0.004192                        0.011976   \n",
       "11                  0.003593                        0.010778   \n",
       "12                  0.002994                        0.008982   \n",
       "13                  0.002395                        0.007186   \n",
       "14                  0.001198                        0.006587   \n",
       "15                  0.000000                        0.005988   \n",
       "16                  0.000000                        0.004192   \n",
       "17                  0.000000                        1.000000   \n",
       "\n",
       "    costsensitiverandomforest_specificity  \\\n",
       "19                               1.000000   \n",
       "18                               1.000000   \n",
       "0                                0.025749   \n",
       "1                                0.016766   \n",
       "2                                0.013174   \n",
       "3                                0.011377   \n",
       "4                                0.008982   \n",
       "5                                0.007784   \n",
       "6                                0.007186   \n",
       "7                                0.007186   \n",
       "8                                0.006587   \n",
       "9                                0.005988   \n",
       "10                               0.004790   \n",
       "11                               0.004790   \n",
       "12                               0.003593   \n",
       "13                               0.001796   \n",
       "14                               0.000000   \n",
       "15                               0.000000   \n",
       "16                               0.000000   \n",
       "17                               0.000000   \n",
       "\n",
       "    costsensitivelogisticregression_precision  \\\n",
       "19                                   0.500000   \n",
       "18                                   0.500000   \n",
       "0                                    0.617021   \n",
       "1                                    0.617021   \n",
       "2                                    0.617021   \n",
       "3                                    0.630435   \n",
       "4                                    0.630435   \n",
       "5                                    0.630435   \n",
       "6                                    0.630435   \n",
       "7                                    0.630435   \n",
       "8                                    0.630435   \n",
       "9                                    0.630435   \n",
       "10                                   0.630435   \n",
       "11                                   0.630435   \n",
       "12                                   0.626374   \n",
       "13                                   0.633333   \n",
       "14                                   0.633333   \n",
       "15                                   0.633333   \n",
       "16                                   0.633333   \n",
       "17                                   1.000000   \n",
       "\n",
       "    costsensitivelogisticregression_recall  \\\n",
       "19                                1.000000   \n",
       "18                                0.800000   \n",
       "0                                 0.698795   \n",
       "1                                 0.698795   \n",
       "2                                 0.698795   \n",
       "3                                 0.698795   \n",
       "4                                 0.698795   \n",
       "5                                 0.698795   \n",
       "6                                 0.698795   \n",
       "7                                 0.698795   \n",
       "8                                 0.698795   \n",
       "9                                 0.698795   \n",
       "10                                0.698795   \n",
       "11                                0.698795   \n",
       "12                                0.686747   \n",
       "13                                0.686747   \n",
       "14                                0.686747   \n",
       "15                                0.686747   \n",
       "16                                0.686747   \n",
       "17                                0.000000   \n",
       "\n",
       "    costsensitivelogisticregression_specificity  \n",
       "19                                     1.000000  \n",
       "18                                     1.000000  \n",
       "0                                      0.021557  \n",
       "1                                      0.021557  \n",
       "2                                      0.021557  \n",
       "3                                      0.020359  \n",
       "4                                      0.020359  \n",
       "5                                      0.020359  \n",
       "6                                      0.020359  \n",
       "7                                      0.020359  \n",
       "8                                      0.020359  \n",
       "9                                      0.020359  \n",
       "10                                     0.020359  \n",
       "11                                     0.020359  \n",
       "12                                     0.020359  \n",
       "13                                     0.019760  \n",
       "14                                     0.019760  \n",
       "15                                     0.019760  \n",
       "16                                     0.019760  \n",
       "17                                     0.000000  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add rows\n",
    "row1 = {'cutoff': 1, \n",
    "       'randomforest_precision': 1.0, 'randomforest_recall': 0, 'randomforest_specificity': 0,\n",
    "      'logisticregression_precision': 1.0, 'logisticregression_recall': 0, 'logisticregression_specificity': 1,\n",
    "      'costsensitiverandomforest_precision': 1.0, 'costsensitiverandomforest_recall': 0, \n",
    "        'costsensitiverandomforest_specificity': 0, 'costsensitivelogisticregression_precision': 1.0,\n",
    "        'costsensitivelogisticregression_recall': 0, 'costsensitivelogisticregression_specificity': 0\n",
    "       }\n",
    "\n",
    "row2 = {'cutoff': 0.05, \n",
    "       'randomforest_precision': .6, 'randomforest_recall': .88, 'randomforest_specificity': 1,\n",
    "      'logisticregression_precision': .53, 'logisticregression_recall': .85, 'logisticregression_specificity': 1,\n",
    "      'costsensitiverandomforest_precision': .57, 'costsensitiverandomforest_recall': .9,\n",
    "        'costsensitiverandomforest_specificity': 1, 'costsensitivelogisticregression_precision': .5,\n",
    "       'costsensitivelogisticregression_recall': .8, 'costsensitivelogisticregression_specificity': 1}\n",
    "\n",
    "row3 = {'cutoff': 0, \n",
    "       'randomforest_precision': .6, 'randomforest_recall': 1, 'randomforest_specificity': 1,\n",
    "      'logisticregression_precision': .5, 'logisticregression_recall': 1, 'logisticregression_specificity': 1,\n",
    "      'costsensitiverandomforest_precision': .55, 'costsensitiverandomforest_recall': 1,\n",
    "       'costsensitiverandomforest_specificity': 1, 'costsensitivelogisticregression_precision': .5,\n",
    "       'costsensitivelogisticregression_recall': 1, 'costsensitivelogisticregression_specificity': 1}\n",
    "\n",
    "\n",
    "res_preds2 = res_preds.append(row1, ignore_index=True)\n",
    "res_preds2 = res_preds2.append(row2, ignore_index=True)\n",
    "res_preds2 = res_preds2.append(row3, ignore_index=True)\n",
    "res_preds2.sort_values(by='cutoff', inplace=True)\n",
    "res_preds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x10dd36908>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfkAAAF3CAYAAACmDDJMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npZRyUxrklVJKKTelQV4ppZRyU/8fdi4uO5FIdBcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d989c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.plot(res_preds2['randomforest_recall'], res_preds2['randomforest_precision'],\n",
    "         color='black', \n",
    "        label='Bosque Aleatorio')\n",
    "\n",
    "#ax.plot(res_preds['randomforest_recall'], res_preds['randomforest_specificity'],\n",
    "        # color='black',\n",
    "#        label='Bosque Aleatorio')\n",
    "\n",
    "ax.plot(res_preds2['logisticregression_recall'], res_preds2['logisticregression_precision'],\n",
    "         color='lightsteelblue',  label = 'Regresin Logstica')\n",
    "        # where='post')\n",
    "\n",
    "ax.plot(res_preds2['costsensitiverandomforest_recall'], res_preds2['costsensitiverandomforest_precision'],\n",
    "         color='grey', label = 'Bosque Aleatorio Costo-Sensible')\n",
    "        # where='post')\n",
    "ax.plot(res_preds2['costsensitivelogisticregression_recall'], \n",
    "        res_preds2['costsensitivelogisticregression_precision'],\n",
    "          label = 'Regresin Logstica Costo-Sensible', alpha = .5)\n",
    "\n",
    "ax.set_ylim([0,1.05])\n",
    "ax.set_xlim([0,1])\n",
    "#ax.set_title('Curva Precisin-Sensibilidad', size=20)\n",
    "\n",
    "ax.set_xlabel('Sensibilidad', size=10)\n",
    "ax.set_ylabel('Precisin', size=10)\n",
    "ax.legend(loc=3, prop={'size': 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x107564278>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAesAAAFpCAYAAAC8iwByAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wEG/W8JGzGStAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10741f7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.plot(res_preds['randomforest_specificity'], res_preds['randomforest_recall'],\n",
    "         color='black', \n",
    "        label='Bosque Aleatorio')\n",
    "\n",
    "ax.plot(res_preds['logisticregression_specificity'], res_preds['logisticregression_recall'],\n",
    "         color='lightsteelblue',  label = 'Regresin Logstica')\n",
    "\n",
    "\n",
    "ax.plot(res_preds['costsensitiverandomforest_specificity'], res_preds['costsensitiverandomforest_recall'],\n",
    "         color='grey', label = 'Bosque Aleatorio Costo-Sensible')\n",
    "\n",
    "ax.set_ylim([0,1.05])\n",
    "ax.set_xlim([0,1])\n",
    "#ax.set_title('Curva Precisin-Sensibilidad', size=20)\n",
    "\n",
    "#ax.set_xlabel('Sensibilidad', size=10)\n",
    "#ax.set_ylabel('Precisin', size=10)\n",
    "ax.legend(loc=3, prop={'size': 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
